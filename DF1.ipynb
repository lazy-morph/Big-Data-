{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sc.parallelize([\n",
    "(1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD'\n",
    ", 13.75, 9.48, 0.61, 4.02)\n",
    ", (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD'\n",
    ", 11.04, 7.74, 0.52, 2.03)\n",
    ", (3, 'MacBook Air', 2016, '13.3\"', '8GB', '128GB SSD'\n",
    ", 12.8, 8.94, 0.68, 2.96)\n",
    ", (4, 'iMac', 2017, '27\"', '64GB', '1TB SSD'\n",
    ", 25.6, 8.0, 20.3, 20.8)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_df = spark.createDataFrame(sample_data,['Id'\n",
    ", 'Model'\n",
    ", 'Year'\n",
    ", 'ScreenSize'\n",
    ", 'RAM'\n",
    ", 'HDD'\n",
    ", 'W'\n",
    ", 'D'\n",
    ", 'H'\n",
    ", 'Weight'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as f\n",
    "sample_data_transformed = (\n",
    "sample_data_df\n",
    ".rdd\n",
    ".map(lambda row: sql.Row(**row.asDict(),\n",
    "HDD_size=row.HDD.split(' ')[0]\n",
    ")\n",
    ")\n",
    ".map(lambda row: sql.Row(**row.asDict(),\n",
    "    HDD_type=row.HDD.split(' ')[1]\n",
    ")\n",
    ")\n",
    ".map(lambda row: sql.Row(**row.asDict(),\n",
    "Volume=row.H * row.D * row.W\n",
    ")\n",
    ")\n",
    ".toDF()\n",
    ".select(\n",
    "sample_data_df.columns +\n",
    "[\n",
    "'HDD_size'\n",
    ", 'HDD_type'\n",
    ", f.round(\n",
    "f.col('Volume')\n",
    ").alias('Volume_cuIn')\n",
    "]\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+--------+--------+-----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|HDD_size|HDD_type|Volume_cuIn|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+--------+--------+-----------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|   512GB|     SSD|       80.0|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|   256GB|     SSD|       44.0|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|   128GB|     SSD|       78.0|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|     1TB|     SSD|     4157.0|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                val|\n",
      "+---+-------------------+\n",
      "|  0| 0.5944708698472357|\n",
      "|  1|0.34362507517305874|\n",
      "|  2| 0.2501750762673367|\n",
      "+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+-------------------+-------------------+\n",
      "| id|                val|        probability|\n",
      "+---+-------------------+-------------------+\n",
      "|  0| 0.5944708698472357|0.33432679348128996|\n",
      "|  1|0.34362507517305874|0.37607088502856584|\n",
      "|  2| 0.2501750762673367| 0.3866511871448149|\n",
      "|  3| 0.2564542630918356| 0.3860366647096993|\n",
      "|  4|0.21242539500233737| 0.3900420179121062|\n",
      "+---+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "big_df = (\n",
    "spark\n",
    ".range(0, 1000000)\n",
    ".withColumn('val', f.rand())\n",
    ")\n",
    "big_df.cache()\n",
    "big_df.show(3)\n",
    "@f.pandas_udf('double', f.PandasUDFType.SCALAR)\n",
    "def pandas_pdf(v):\n",
    "    return pd.Series(stats.norm.pdf(v))\n",
    "(\n",
    "big_df\n",
    ".withColumn('probability', pandas_pdf(big_df.val))\n",
    ".show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
